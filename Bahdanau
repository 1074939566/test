import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import jieba_fast
from collections import Counter
import time
from nltk.translate.bleu_score import corpus_bleu
import matplotlib.pyplot as plt
import random
from tqdm import tqdm
from torch.cuda.amp import GradScaler, autocast
from sklearn.model_selection import train_test_split

# 设置随机种子
random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)

# 设备配置
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# --------------------- 数据预处理 ---------------------
class BilingualDataset(Dataset):
    def __init__(self, src_file, tgt_file, src_vocab, tgt_vocab, max_len=50, sample_ratio=1, max_samples=10000):
        with open(src_file, 'r', encoding='utf-8') as f_src, open(tgt_file, 'r', encoding='utf-8') as f_tgt:
            src_lines = [line.strip() for line in f_src]
            tgt_lines = [line.strip() for line in f_tgt]
        
        combined = list(zip(src_lines, tgt_lines))
        random.shuffle(combined)
        sampled = combined[:min(int(len(combined)*sample_ratio), max_samples)]
        self.src_data, self.tgt_data = zip(*sampled)
        
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
        self.max_len = max_len

        # 预编码
        self.src_encoded = []
        self.tgt_encoded = []
        for src, tgt in zip(self.src_data, self.tgt_data):
            src_tokens = ['<sos>'] + src.lower().split()[:self.max_len-2] + ['<eos>']
            src_ids = [self.src_vocab.get(token, self.src_vocab['<unk>']) for token in src_tokens]
            
            tgt_tokens = ['<sos>'] + list(jieba_fast.cut(tgt))[:self.max_len-2] + ['<eos>']
            tgt_ids = [self.tgt_vocab.get(token, self.tgt_vocab['<unk>']) for token in tgt_tokens]
            
            self.src_encoded.append(torch.tensor(src_ids, dtype=torch.long))
            self.tgt_encoded.append(torch.tensor(tgt_ids, dtype=torch.long))

    def __len__(self):
        return len(self.src_data)

    def __getitem__(self, idx):
        return self.src_encoded[idx], self.tgt_encoded[idx]

def build_vocab(file_path, tokenizer, max_vocab_size=5000):
    counter = Counter()
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f, desc=f"Building vocab from {file_path}"):
            tokens = tokenizer(line.strip())
            counter.update(tokens)
    
    vocab = {'<pad>':0, '<sos>':1, '<eos>':2, '<unk>':3}
    for token, _ in counter.most_common(max_vocab_size):
        vocab[token] = len(vocab)
    return vocab

# --------------------- 模型定义 ---------------------
class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, bidirectional=True)
        self.fc_hidden = nn.Linear(hid_dim * 2, hid_dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src):
        embedded = self.dropout(self.embedding(src))  # [src_len, batch, emb_dim]
        outputs, hidden = self.rnn(embedded)
        
        # 合并双向状态
        hidden = self.fc_hidden(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)).unsqueeze(0)
        
        return outputs, hidden

class Attention(nn.Module):
    def __init__(self, hid_dim):
        super().__init__()
        self.attn = nn.Linear((hid_dim * 2) + hid_dim, hid_dim)
        self.v = nn.Linear(hid_dim, 1, bias=False)
        
    def forward(self, hidden, encoder_outputs):
        # hidden: [batch_size, hid_dim]
        # encoder_outputs: [src_len, batch_size, hid_dim * 2]
        
        src_len = encoder_outputs.shape[0]
        
        hidden = hidden.repeat(src_len, 1, 1).permute(1, 0, 2)
        encoder_outputs = encoder_outputs.permute(1, 0, 2)
        
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))
        attention = self.v(energy).squeeze(2)
        
        return F.softmax(attention, dim=1)

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):
        super().__init__()
        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.attention = attention
        self.rnn = nn.GRU((hid_dim * 2) + emb_dim, hid_dim, n_layers, dropout=dropout)
        self.fc_out = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input, hidden, encoder_outputs):
        # input: [batch_size]
        # hidden: [n_layers, batch_size, hid_dim]
        # encoder_outputs: [src_len, batch_size, hid_dim * 2]
        
        input = input.unsqueeze(0)  # [1, batch_size]
        embedded = self.dropout(self.embedding(input))  # [1, batch_size, emb_dim]
        
        a = self.attention(hidden[-1], encoder_outputs)  # [batch_size, src_len]
        a = a.unsqueeze(1)  # [batch_size, 1, src_len]
        
        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # [batch_size, src_len, hid_dim * 2]
        weighted = torch.bmm(a, encoder_outputs)  # [batch_size, 1, hid_dim * 2]
        weighted = weighted.permute(1, 0, 2)  # [1, batch_size, hid_dim * 2]
        
        rnn_input = torch.cat((embedded, weighted), dim=2)  # [1, batch_size, (hid_dim * 2) + emb_dim]
        
        output, hidden = self.rnn(rnn_input, hidden)
        
        embedded = embedded.squeeze(0)
        output = output.squeeze(0)
        weighted = weighted.squeeze(0)
        
        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))
        
        return prediction, hidden, a.squeeze(1)

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        trg_len = trg.shape[0]
        batch_size = trg.shape[1]
        trg_vocab_size = self.decoder.output_dim
        
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        
        encoder_outputs, hidden = self.encoder(src)
        
        # 确保 hidden 的尺寸正确
        hidden = hidden.repeat(self.decoder.n_layers, 1, 1)
        
        input = trg[0,:]
        
        for t in range(1, trg_len):
            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)
            outputs[t] = output
            teacher_force = random.random() < teacher_forcing_ratio
            input = trg[t] if teacher_force else output.argmax(1)
        
        return outputs

# --------------------- 训练工具 ---------------------
def collate_fn(batch):
    src_batch, tgt_batch = zip(*batch)
    src_batch = pad_sequence(src_batch, padding_value=0, batch_first=True)
    tgt_batch = pad_sequence(tgt_batch, padding_value=0, batch_first=True)
    return src_batch.transpose(0, 1), tgt_batch.transpose(0, 1)

def train(model, iterator, optimizer, criterion, scaler, config):
    model.train()
    epoch_loss = 0
    
    for src, trg in tqdm(iterator, desc="训练中", leave=False):
        src, trg = src.to(device), trg.to(device)
        optimizer.zero_grad(set_to_none=True)
        
        with autocast():
            output = model(src, trg[:-1], config['teacher_forcing_ratio'])
            loss = criterion(output.reshape(-1, output.shape[-1]), trg[1:].reshape(-1))
        
        scaler.scale(loss).backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), config['grad_clip'])
        scaler.step(optimizer)
        scaler.update()
        
        epoch_loss += loss.item()
    
    return epoch_loss / len(iterator)

def evaluate(model, iterator, criterion):
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for src, trg in tqdm(iterator, desc="评估中", leave=False):
            src, trg = src.to(device), trg.to(device)
            output = model(src, trg[:-1], 0)  # 评估时不使用teacher forcing
            loss = criterion(output.reshape(-1, output.shape[-1]), trg[1:].reshape(-1))
            epoch_loss += loss.item()
    return epoch_loss / len(iterator)

def calculate_bleu(model, iterator, tgt_vocab, max_len=50, n_grams=4):
    targets, predictions = [], []
    model.eval()
    with torch.no_grad():
        for src, trg in tqdm(iterator, desc="计算BLEU分数", leave=False):
            src = src.to(device)
            trg = trg.to(device)
            
            encoder_outputs, hidden = model.encoder(src)
            
            # 初始输入是<sos>
            input = torch.ones(src.size(1), dtype=torch.long).to(device) * tgt_vocab['<sos>']
            
            outputs = torch.zeros(max_len, src.size(1), dtype=torch.long).to(device)
            
            for t in range(1, max_len):
                output, hidden, _ = model.decoder(input, hidden, encoder_outputs)
                top1 = output.argmax(1)
                outputs[t] = top1
                input = top1
            
            # 处理预测结果
            predictions_batch = []
            for pred in outputs[1:].t():  # 跳过<sos>
                pred_sentence = []
                for token in pred:
                    if token.item() == tgt_vocab['<eos>']:
                        break
                    pred_sentence.append(str(token.item()))
                predictions_batch.append(pred_sentence)
            
            # 处理目标结果
            targets_batch = []
            for tgt in trg.t():
                tgt_sentence = []
                for token in tgt[1:]:  # 跳过<sos>
                    if token.item() == tgt_vocab['<eos>']:
                        break
                    tgt_sentence.append(str(token.item()))
                targets_batch.append([tgt_sentence])  # NLTK需要列表的列表
            
            predictions.extend(predictions_batch)
            targets.extend(targets_batch)
    
    weights = tuple((1./n_grams for _ in range(n_grams)))
    return corpus_bleu(targets, predictions, weights=weights)

# --------------------- 主程序 ---------------------
if __name__ == "__main__":
    # 配置参数
    config = {
        'enc_emb_dim': 256,
        'dec_emb_dim': 256,
        'hid_dim': 512,
        'enc_dropout': 0.3,
        'dec_dropout': 0.3,
        'n_layers': 2,
        'batch_size': 64,
        'n_epochs': 20,
        'learning_rate': 0.001,  # 调高学习率
        'grad_clip': 1.0,
        'teacher_forcing_ratio': 0.5,
        'sample_ratio': 1.0,  # 增加采样比例
        'max_samples': 50000  # 增加最大样本数量
    }
    
    # 构建词汇表
    print("构建词汇表中...")
    en_tokenizer = lambda x: x.lower().split()
    zh_tokenizer = lambda x: list(jieba_fast.cut(x))
    
    en_vocab = build_vocab('WikiTitles.en-zh.en', en_tokenizer)
    zh_vocab = build_vocab('WikiTitles.en-zh.zh', zh_tokenizer)
    
    # 数据加载
    print("准备数据加载器...")
    train_dataset = BilingualDataset(
        'WikiTitles.en-zh.en', 'WikiTitles.en-zh.zh', 
        en_vocab, zh_vocab,
        sample_ratio=config['sample_ratio'],
        max_samples=config['max_samples']
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=4,
        pin_memory=True
    )
    
    # 初始化模型
    print("初始化模型中...")
    attention = Attention(config['hid_dim'])
    encoder = Encoder(
        len(en_vocab),
        config['enc_emb_dim'],
        config['hid_dim'],
        config['n_layers'],
        config['enc_dropout']
    )
    decoder = Decoder(
        len(zh_vocab),
        config['dec_emb_dim'],
        config['hid_dim'],
        config['n_layers'],
        config['dec_dropout'],
        attention
    )
    model = Seq2Seq(encoder, decoder, device).to(device)
    
    # 打印参数量
    def count_parameters(model):
        return sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"模型参数量: {count_parameters(model):,}")
    
    # 优化器和损失函数
    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])  # 使用AdamW优化器
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)
    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)
    scaler = GradScaler(enabled=torch.cuda.is_available())
    
    # 训练循环
    print("开始训练...")
    best_bleu = 0
    train_losses, val_losses, bleus = [], [], []
    start_time = time.time()
    
    for epoch in range(config['n_epochs']):
        train_loss = train(model, train_loader, optimizer, criterion, scaler, config)
        val_loss = evaluate(model, train_loader, criterion)
        bleu = calculate_bleu(model, train_loader, zh_vocab)
        
        scheduler.step(val_loss)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        bleus.append(bleu)
        
        if bleu > best_bleu:
            best_bleu = bleu
            torch.save(model.state_dict(), 'best_model.pt')
        
        epoch_time = time.time() - start_time
        print(f"Epoch: {epoch+1:02} | 用时: {epoch_time:.2f}s")
        print(f"\t训练损失: {train_loss:.3f} | 验证损失: {val_loss:.3f} | BLEU-4分数: {bleu:.3f}")
    
    # 训练结果
    total_time = time.time() - start_time
    print(f"\n训练完成! 总用时: {total_time//60:.0f}m {total_time%60:.0f}s")
    print(f"最佳BLEU-4分数: {best_bleu:.3f}")
    
    # 绘制曲线
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='训练损失')
    plt.plot(val_losses, label='验证损失')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(bleus, label='BLEU-4')
    plt.xlabel('Epoch')
    plt.ylabel('Score')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('training_curves.png')
    plt.show()
