综合任务：机器翻译（中-英）建模与模型对比 
任务目标 
设计并实现三种不同的神经网络模型，完成中文到英文的机器翻译任务，并对模型性能进行对比分析。
要求包含以下模型： 
1. 基于RNN的Seq2Seq模型（无注意力机制） 
2. 基于Bahdanau注意力的Seq2Seq模型 
3. Transformer模型 
1. 数据准备 
数据集：使用上次下发的中英平行。 
 2. 模型设计与实现 
模型1：基于RNN的Seq2Seq模型 
编码器：双向LSTM，输出上下文向量。 
解码器：单向LSTM，依赖上下文向量生成目标序列。
模型2：基于Bahdanau注意力的Seq2Seq模型 
编码器：输出所有时间步的隐藏状态。 
解码器：使用加性注意力动态聚焦编码器的隐藏状态。
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
    def forward(self, src, trg):
        encoder_outputs, hidden = self.encoder(src)
        output = self.decoder(trg, hidden)
        return output
# 编码器（双向LSTM）
class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, 
bidirectional=True)
        
    def forward(self, src):
        embedded = self.embedding(src)
        outputs, (hidden, cell) = self.rnn(embedded)
        return outputs, (hidden, cell)
# 解码器（单向LSTM）
class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)
        self.fc = nn.Linear(hid_dim, output_dim)
        
    def forward(self, trg, hidden):
        embedded = self.embedding(trg)
        output, (hidden, cell) = self.rnn(embedded, hidden)
        prediction = self.fc(output)
        return prediction
class AttnSeq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder模型3：Transformer模型 
编码器：多层多头自注意力 + 前馈网络。 
解码器：掩蔽多头自注意力 + 编码器-解码器注意力。
    def forward(self, src, trg):
        encoder_outputs, hidden = self.encoder(src)
        output, attention = self.decoder(trg, hidden, encoder_outputs)
        return output, attention
# 注意力解码器
class AttnDecoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.attention = AdditiveAttention(hid_dim, hid_dim, hid_dim, dropout)
        self.rnn = nn.LSTM(emb_dim + hid_dim, hid_dim, n_layers, dropout=dropout)
        self.fc = nn.Linear(hid_dim, output_dim)
        
    def forward(self, trg, hidden, encoder_outputs):
        trg = self.embedding(trg)
        query = hidden[-1].unsqueeze(1)  # 使用最后一个隐藏层作为查询
        context = self.attention(query, encoder_outputs, encoder_outputs)
        rnn_input = torch.cat((trg, context), dim=2)
        output, (hidden, cell) = self.rnn(rnn_input, hidden)
        prediction = self.fc(output)
        return prediction, context
class Transformer(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
    def forward(self, src, trg):
        enc_output = self.encoder(src)
        output = self.decoder(trg, enc_output)
        return output
# 编码器层（参考文档中的实现）
class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)
        self.ffn = PositionWiseFFN(d_model, dim_feedforward, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    def forward(self, src, src_mask=None):
        src2 = self.self_attn(src, src, src, src_mask)
        src = self.norm1(src + self.dropout(src2))
        src2 = self.ffn(src)
        src = self.norm2(src + self.dropout(src2))模型 训练损失 验证损失 BLEU-4 训练时间（小时）
RNN Seq2Seq        
Attn Seq2Seq        
Transformer        
3. 训练与评估 
训练配置 
优化器：Adam ( lr=0.001 , betas=(0.9, 0.98) ) 
损失函数：交叉熵损失（忽略填充词元） 
评估指标：BLEU-4分数 
训练代码示例 
性能对比 
4. 可视化与分析 
注意力权重可视化（以Transformer为例） 
        return src
def train(model, iterator, optimizer, criterion):
    model.train()
    epoch_loss = 0
    for batch in iterator:
        src = batch.src
        trg = batch.trg
        optimizer.zero_grad()
        output = model(src, trg[:, :-1])  # 忽略<eos>
        loss = criterion(output.reshape(-1, output.shape[-1]), trg[:, 
1:].reshape(-1))
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)
        optimizer.step()
        epoch_loss += loss.item()
    return epoch_loss / len(iterator)
# 编码器自注意力热图
enc_attention = model.encoder.layers[0].self_attn.attention_weights[0]
d2l.show_heatmaps(enc_attention.cpu(), xlabel='Key', ylabel='Query', titles=
['Head 1'])
# 解码器-编码器注意力热图
dec_enc_attention = model.decoder.layers[0].cross_attn.attention_weights[0]
d2l.show_heatmaps(dec_enc_attention.cpu(), xlabel='Key', ylabel='Query', titles=
['Head 1'])分析结论 
1. RNN Seq2Seq：
2. Attn Seq2Seq：
3. Transformer：
5. 总结 
Transformer在翻译任务中表现最优，适合处理长序列和复杂依赖。 
注意力机制能有效捕捉跨语言对齐信息，但对计算资源要求较高。 
实际应用中可根据任务需求（速度 vs. 精度）选择模型。