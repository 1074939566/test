# --------------------- 主程序 ---------------------
if __name__ == "__main__":
    # 配置参数
    config = {
        'emb_dim': 256,
        'nhead': 8,
        'num_encoder_layers': 3,
        'num_decoder_layers': 3,
        'dim_feedforward': 512,
        'dropout': 0.3,
        'batch_size': 64,
        'n_epochs': 20,
        'learning_rate': 0.001,  # 调高学习率
        'grad_clip': 1.0,
        'sample_ratio': 1.0,  # 增加采样比例
        'max_samples': 50000  # 增加最大样本数量
    }
    
    # 构建词汇表
    print("构建词汇表中...")
    en_tokenizer = lambda x: x.lower().split()
    zh_tokenizer = lambda x: list(jieba_fast.cut(x))
    
    en_vocab = build_vocab('WikiTitles.en-zh.en', en_tokenizer)
    zh_vocab = build_vocab('WikiTitles.en-zh.zh', zh_tokenizer)
    
    # 数据加载
    print("准备数据加载器...")
    train_dataset = BilingualDataset(
        'WikiTitles.en-zh.en', 'WikiTitles.en-zh.zh', 
        en_vocab, zh_vocab,
        sample_ratio=config['sample_ratio'],
        max_samples=config['max_samples']
    )
    
    # 数据分割：增加验证集
    train_data, val_data = train_test_split(
        list(zip(train_dataset.src_encoded, train_dataset.tgt_encoded)), 
        test_size=0.1, random_state=42
    )

    # 创建训练集DataLoader
    train_loader = DataLoader(
        torch.utils.data.TensorDataset(
            pad_sequence([x[0] for x in train_data], padding_value=0).transpose(0, 1),
            pad_sequence([x[1] for x in train_data], padding_value=0).transpose(0, 1)
        ),
        batch_size=config['batch_size'],
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=8,  # 增加num_workers以并行加载数据
        pin_memory=True
    )

    # 创建验证集DataLoader
    val_loader = DataLoader(
        torch.utils.data.TensorDataset(
            pad_sequence([x[0] for x in val_data], padding_value=0).transpose(0, 1),
            pad_sequence([x[1] for x in val_data], padding_value=0).transpose(0, 1)
        ),
        batch_size=config['batch_size'] * 2,
        collate_fn=collate_fn
    )
    
    # 初始化模型
    print("初始化模型中...")
    model = TransformerModel(
        src_vocab_size=len(en_vocab),
        tgt_vocab_size=len(zh_vocab),
        emb_dim=config['emb_dim'],
        nhead=config['nhead'],
        num_encoder_layers=config['num_encoder_layers'],
        num_decoder_layers=config['num_decoder_layers'],
        dim_feedforward=config['dim_feedforward'],
        dropout=config['dropout']
    ).to(device)
    
    # 打印参数量
    def count_parameters(model):
        return sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"模型参数量: {count_parameters(model):,}")
    
    # 优化器和损失函数
    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])  # 使用AdamW优化器
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)
    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)
    scaler = GradScaler(enabled=torch.cuda.is_available())
    
    # 训练循环
    print("开始训练...")
    best_bleu = 0
    best_epoch = 0
    train_losses, val_losses, bleu_scores = [], [], []
    start_time = time.time()
    
    for epoch in range(config['n_epochs']):
        train_loss = train(model, train_loader, optimizer, criterion, scaler, pad_idx=0)
        val_loss = evaluate(model, val_loader, criterion, pad_idx=0)
        
        scheduler.step(val_loss)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        # 计算当前轮的BLEU分数
        bleu = fast_calculate_bleu(model, val_loader, zh_vocab)
        bleu_scores.append(bleu)
        if bleu > best_bleu:
            best_bleu = bleu
            best_epoch = epoch + 1
            torch.save(model.state_dict(), 'best_model.pth')
        
        epoch_time = time.time() - start_time
        print(f"Epoch: {epoch+1:02} | 用时: {epoch_time:.2f}s")
        print(f"\t训练损失: {train_loss:.3f} | 验证损失: {val_loss:.3f} | 当前BLEU: {bleu:.3f}")

    # 训练结果
    total_time = time.time() - start_time
    print(f"\n训练完成! 总用时: {total_time//60:.0f}m {total_time%60:.0f}s")
    print(f"最佳BLEU-4分数: {best_bleu:.3f} (在第 {best_epoch} 轮)")
    
    # 绘制曲线
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='训练损失')
    plt.plot(val_losses, label='验证损失')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    plt.plot(bleu_scores, label='BLEU-4')
    plt.xlabel('Epoch')
    plt.ylabel('Score')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('training_curves.png')
    plt.show()
